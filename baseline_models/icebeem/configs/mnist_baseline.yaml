# config file for baseline (of no transfer) example on MNIST

training:
  batch_size: 63
  n_epochs: 5
  n_iters: 200001
  ngpu: 1
  snapshot_freq: 50
  algo: 'dsm'
  anneal_power: 2.0

data:
  dataset: "MNIST_transferBaseline"
  image_size: 28
  channels: 1
  logit_transform: false
  random_flip: false
  store_loss: true

model:
  sigma_begin: 1
  sigma_end: 0.01
  num_classes: 10
  ngf: 64
  final_layer: true
  feature_size: 90
  augment: false
  positive: false
  architecture: 'ConvMLP'

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: false

n_labels: 8
